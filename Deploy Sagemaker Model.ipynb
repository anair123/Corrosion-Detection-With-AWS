{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e9aec7-fba5-4cf2-bbcc-92181946b85f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import os\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import tensorflow as tf\n",
    "from io import BytesIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "36df3f2b-6f44-4005-91af-153c4d65e0ba",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train directory exists: True\n",
      "Validation directory exists: True\n",
      "Test directory exists: True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Define local directories (assumed to be within the same instance as the notebook)\n",
    "base_dir = os.getcwd()\n",
    "\n",
    "# Local directories for train, validation, and test datasets\n",
    "local_train_dir = os.path.join(base_dir, 'data/train')\n",
    "local_valid_dir = os.path.join(base_dir, 'data/valid')\n",
    "local_test_dir = os.path.join(base_dir, 'data/test')\n",
    "\n",
    "# Verify that the directories exist (this is optional but helpful for debugging)\n",
    "print(f\"Train directory exists: {os.path.exists(local_train_dir)}\")\n",
    "print(f\"Validation directory exists: {os.path.exists(local_valid_dir)}\")\n",
    "print(f\"Test directory exists: {os.path.exists(local_test_dir)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6f148e00-0610-478f-87d4-0908523c4c15",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed /home/ec2-user/SageMaker/data/train/.ipynb_checkpoints\n",
      "Removed /home/ec2-user/SageMaker/data/valid/.ipynb_checkpoints\n",
      "Removed /home/ec2-user/SageMaker/data/test/.ipynb_checkpoints\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "\n",
    "# Function to remove .ipynb_checkpoints folders\n",
    "def remove_ipynb_checkpoints(directory):\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for dir_name in dirs:\n",
    "            if dir_name == '.ipynb_checkpoints':\n",
    "                dir_path = os.path.join(root, dir_name)\n",
    "                shutil.rmtree(dir_path)  # Remove the directory\n",
    "                print(f\"Removed {dir_path}\")\n",
    "\n",
    "# Clean train, valid, and test directories from .ipynb_checkpoints\n",
    "remove_ipynb_checkpoints(local_train_dir)\n",
    "remove_ipynb_checkpoints(local_valid_dir)\n",
    "remove_ipynb_checkpoints(local_test_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e41e0ef5-e932-48d6-8b5a-28dfb52378f8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 270 images belonging to 2 classes.\n",
      "Found 14 images belonging to 2 classes.\n",
      "Found 8 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define image data generator with preprocessing and augmentation for the training set\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,  # Normalize pixel values to [0, 1]\n",
    "    rotation_range=30,  # Augmentation: randomly rotate images\n",
    "    width_shift_range=0.2,  # Augmentation: randomly shift images horizontally\n",
    "    height_shift_range=0.2,  # Augmentation: randomly shift images vertically\n",
    "    shear_range=0.2,  # Augmentation: randomly shear images\n",
    "    zoom_range=0.2,  # Augmentation: randomly zoom into images\n",
    "    horizontal_flip=True,  # Augmentation: randomly flip images horizontally\n",
    "    fill_mode='nearest'  # Fill mode for missing pixels after augmentation\n",
    ")\n",
    "\n",
    "# No augmentation for validation and test sets, only rescaling\n",
    "valid_datagen = ImageDataGenerator(rescale=1./255)\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# Load all datasets from the subdirectories using flow_from_directory\n",
    "train_data = train_datagen.flow_from_directory(\n",
    "    local_train_dir, \n",
    "    target_size=(150, 150),  # Resize images to 150x150 pixels\n",
    "    batch_size=32,\n",
    "    class_mode='binary'  # Binary classification (Corrosion vs No-corrosion)\n",
    ")\n",
    "\n",
    "valid_data = valid_datagen.flow_from_directory(\n",
    "    local_valid_dir,\n",
    "    target_size=(150, 150),\n",
    "    batch_size=32,\n",
    "    class_mode='binary'\n",
    ")\n",
    "\n",
    "test_data = test_datagen.flow_from_directory(\n",
    "    local_test_dir,\n",
    "    target_size=(150, 150),\n",
    "    batch_size=32,\n",
    "    class_mode='binary'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "106cbd86-2fe2-409e-bc27-7652f561c971",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Corrosion': 0, 'no-corrosion': 1}\n"
     ]
    }
   ],
   "source": [
    "print(train_data.class_indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f372c061-2a89-49fa-ad5e-3f322156c9b1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train directory classes: ['no-corrosion', 'Corrosion']\n",
      "Valid directory classes: ['no-corrosion', 'Corrosion']\n",
      "Test directory classes: ['no-corrosion', 'Corrosion']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Check the contents of each directory\n",
    "print(\"Train directory classes:\", os.listdir(local_train_dir))\n",
    "print(\"Valid directory classes:\", os.listdir(local_valid_dir))\n",
    "print(\"Test directory classes:\", os.listdir(local_test_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff8b365-169c-4703-8fa0-3e7ae15e8262",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "585ddf9c-df2f-47b5-aac8-46c25e0b305b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5ee6385c-a987-4091-a03c-1c14577761fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "# Early stopping to stop training when the validation loss doesn't improve\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e3638dc1-78f2-4a27-8397-d235581a380b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "9/9 [==============================] - 21s 1s/step - loss: 1.7766 - accuracy: 0.8815 - val_loss: 0.5598 - val_accuracy: 0.9286\n",
      "Epoch 2/20\n",
      "9/9 [==============================] - 5s 545ms/step - loss: 0.2575 - accuracy: 0.9704 - val_loss: 0.6071 - val_accuracy: 0.9286\n",
      "Epoch 3/20\n",
      "9/9 [==============================] - 5s 487ms/step - loss: 0.1017 - accuracy: 0.9667 - val_loss: 0.3286 - val_accuracy: 0.9286\n",
      "Epoch 4/20\n",
      "9/9 [==============================] - 5s 489ms/step - loss: 0.0085 - accuracy: 0.9963 - val_loss: 0.7216 - val_accuracy: 0.9286\n",
      "Epoch 5/20\n",
      "9/9 [==============================] - 5s 492ms/step - loss: 0.0263 - accuracy: 0.9926 - val_loss: 0.6735 - val_accuracy: 0.9286\n",
      "Epoch 6/20\n",
      "9/9 [==============================] - 5s 493ms/step - loss: 0.0282 - accuracy: 0.9926 - val_loss: 0.0524 - val_accuracy: 1.0000\n",
      "Epoch 7/20\n",
      "9/9 [==============================] - 5s 492ms/step - loss: 0.0434 - accuracy: 0.9889 - val_loss: 0.2463 - val_accuracy: 0.9286\n",
      "Epoch 8/20\n",
      "9/9 [==============================] - 5s 487ms/step - loss: 0.1151 - accuracy: 0.9778 - val_loss: 0.3341 - val_accuracy: 0.9286\n",
      "Epoch 9/20\n",
      "9/9 [==============================] - 5s 484ms/step - loss: 0.0141 - accuracy: 0.9926 - val_loss: 0.0375 - val_accuracy: 1.0000\n",
      "Epoch 10/20\n",
      "9/9 [==============================] - 5s 490ms/step - loss: 0.0094 - accuracy: 0.9963 - val_loss: 0.0153 - val_accuracy: 1.0000\n",
      "Epoch 11/20\n",
      "9/9 [==============================] - 5s 487ms/step - loss: 0.0464 - accuracy: 0.9926 - val_loss: 0.0253 - val_accuracy: 1.0000\n",
      "Epoch 12/20\n",
      "9/9 [==============================] - 5s 484ms/step - loss: 0.0145 - accuracy: 0.9926 - val_loss: 0.3020 - val_accuracy: 0.7857\n",
      "Epoch 13/20\n",
      "9/9 [==============================] - 5s 497ms/step - loss: 0.0317 - accuracy: 0.9889 - val_loss: 0.1242 - val_accuracy: 0.9286\n",
      "Epoch 14/20\n",
      "9/9 [==============================] - 5s 498ms/step - loss: 0.0079 - accuracy: 0.9963 - val_loss: 0.1131 - val_accuracy: 0.9286\n",
      "Epoch 15/20\n",
      "9/9 [==============================] - 5s 492ms/step - loss: 0.0061 - accuracy: 1.0000 - val_loss: 0.1434 - val_accuracy: 0.9286\n",
      "Epoch 16/20\n",
      "9/9 [==============================] - 5s 501ms/step - loss: 0.0203 - accuracy: 0.9963 - val_loss: 0.2060 - val_accuracy: 0.9286\n",
      "Epoch 17/20\n",
      "9/9 [==============================] - 5s 550ms/step - loss: 0.0105 - accuracy: 1.0000 - val_loss: 0.2279 - val_accuracy: 0.9286\n",
      "Epoch 18/20\n",
      "9/9 [==============================] - 5s 521ms/step - loss: 0.0158 - accuracy: 0.9963 - val_loss: 0.3446 - val_accuracy: 0.9286\n",
      "Epoch 19/20\n",
      "9/9 [==============================] - 5s 497ms/step - loss: 9.4938e-04 - accuracy: 1.0000 - val_loss: 0.4272 - val_accuracy: 0.9286\n",
      "Epoch 20/20\n",
      "9/9 [==============================] - 5s 505ms/step - loss: 0.0027 - accuracy: 1.0000 - val_loss: 0.3585 - val_accuracy: 0.9286\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Dropout\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "\n",
    "# Load the base pre-trained model\n",
    "base_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=(150, 150, 3))\n",
    "\n",
    "# Freeze the base model layers\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Build the model\n",
    "model = Sequential([\n",
    "    base_model,\n",
    "    Flatten(),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(1, activation='sigmoid')  # Output layer for binary classification\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(train_data, validation_data=valid_data, epochs=20, callbacks=[early_stopping])\n",
    "\n",
    "# Optionally, unfreeze some layers and fine-tune the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a71c9f4-6991-4cf9-ad1d-f36c7418324f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-28 05:17:14.348223: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:479] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-09-28 05:17:17.399033: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:10575] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-09-28 05:17:17.432703: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1442] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mPIL\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Image\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msagemaker\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m session, Predictor\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/tensorflow/__init__.py:45\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tf2 \u001b[38;5;28;01mas\u001b[39;00m _tf2\n\u001b[1;32m     43\u001b[0m _tf2\u001b[38;5;241m.\u001b[39menable()\n\u001b[0;32m---> 45\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __internal__\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __operators__\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m audio\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/tensorflow/_api/v2/__internal__/__init__.py:8\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03m\"\"\"Public API for tf._api.v2.__internal__ namespace\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_sys\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m__internal__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m autograph\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m__internal__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m decorator\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m__internal__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dispatch\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/tensorflow/_api/v2/__internal__/autograph/__init__.py:8\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03m\"\"\"Public API for tf._api.v2.__internal__.autograph namespace\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_sys\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mautograph\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mag_ctx\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m control_status_ctx \u001b[38;5;66;03m# line: 34\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mautograph\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimpl\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tf_convert \u001b[38;5;66;03m# line: 493\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/tensorflow/python/autograph/core/ag_ctx.py:21\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01minspect\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mthreading\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mautograph\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ag_logging\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtf_export\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tf_export\n\u001b[1;32m     25\u001b[0m stacks \u001b[38;5;241m=\u001b[39m threading\u001b[38;5;241m.\u001b[39mlocal()\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/tensorflow/python/autograph/utils/__init__.py:17\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# ==============================================================================\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"Utility module that contains APIs usable in the generated code.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mautograph\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcontext_managers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m control_dependency_on_returns\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mautograph\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmisc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m alias_tensors\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mautograph\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtensor_list\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dynamic_list_append\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/tensorflow/python/autograph/utils/context_managers.py:19\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"Various context managers.\"\"\"\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcontextlib\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ops\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tensor_array_ops\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcontrol_dependency_on_returns\u001b[39m(return_value):\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:45\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprotobuf\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m config_pb2\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# pywrap_tensorflow must be imported first to avoid protobuf issues.\u001b[39;00m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# (b/143110113)\u001b[39;00m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# pylint: disable=invalid-import-order,g-bad-import-order,unused-import\u001b[39;00m\n\u001b[0;32m---> 45\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pywrap_tensorflow\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pywrap_tfe\n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m# pylint: enable=invalid-import-order,g-bad-import-order,unused-import\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/tensorflow/python/pywrap_tensorflow.py:34\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mplatform\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m self_check\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# TODO(mdan): Cleanup antipattern: import for side effects.\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# Perform pre-load sanity checks in order to produce a more actionable error.\u001b[39;00m\n\u001b[0;32m---> 34\u001b[0m \u001b[43mself_check\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreload_check\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# pylint: disable=wildcard-import,g-import-not-at-top,unused-import,line-too-long\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     39\u001b[0m   \u001b[38;5;66;03m# This import is expected to fail if there is an explicit shared object\u001b[39;00m\n\u001b[1;32m     40\u001b[0m   \u001b[38;5;66;03m# dependency (with_framework_lib=true), since we do not need RTLD_GLOBAL.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow2_p310/lib/python3.10/site-packages/tensorflow/python/platform/self_check.py:63\u001b[0m, in \u001b[0;36mpreload_check\u001b[0;34m()\u001b[0m\n\u001b[1;32m     50\u001b[0m       \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[1;32m     51\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not find the DLL(s) \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m. TensorFlow requires that these DLLs \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     52\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbe installed in a directory that is named in your \u001b[39m\u001b[38;5;132;01m%%\u001b[39;00m\u001b[38;5;124mPATH\u001b[39m\u001b[38;5;132;01m%%\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     56\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://support.microsoft.com/help/2977003/the-latest-supported-visual-c-downloads\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     57\u001b[0m           \u001b[38;5;241m%\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m or \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(missing))\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     59\u001b[0m   \u001b[38;5;66;03m# Load a library that performs CPU feature guard checking.  Doing this here\u001b[39;00m\n\u001b[1;32m     60\u001b[0m   \u001b[38;5;66;03m# as a preload check makes it more likely that we detect any CPU feature\u001b[39;00m\n\u001b[1;32m     61\u001b[0m   \u001b[38;5;66;03m# incompatibilities before we trigger them (which would typically result in\u001b[39;00m\n\u001b[1;32m     62\u001b[0m   \u001b[38;5;66;03m# SIGILL).\u001b[39;00m\n\u001b[0;32m---> 63\u001b[0m   \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mplatform\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _pywrap_cpu_feature_guard\n\u001b[1;32m     64\u001b[0m   _pywrap_cpu_feature_guard\u001b[38;5;241m.\u001b[39mInfoAboutUnusedCPUFeatures()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import io\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sagemaker import session, Predictor\n",
    "\n",
    "# Assuming you have already deployed your model\n",
    "# Define the endpoint name\n",
    "endpoint_name = \"corrosion-endpoint\"\n",
    "\n",
    "\n",
    "# Create a predictor for the deployed model\n",
    "predictor = Predictor(endpoint_name)\n",
    "\n",
    "# Function to load and preprocess an image from a file path\n",
    "def load_and_preprocess_image(image_path):\n",
    "    try:\n",
    "        img = Image.open(image_path)\n",
    "        if img.mode != 'RGB':\n",
    "            img = img.convert('RGB')\n",
    "        img = img.resize((150, 150))\n",
    "        img_array = np.array(img)\n",
    "        img_array = img_array.astype(np.float32) / 255.0\n",
    "        img_array = np.expand_dims(img_array, axis=0)\n",
    "        return img_array\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading image: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "543c6aa4-2265-4e6b-a9f5-56d071ff3fb5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Example usage\n",
    "image_path = \"images/pipe.jpg\"  # Replace with your actual image path\n",
    "img = load_and_preprocess_image(image_path=image_path) \n",
    "pred_value = round(model.predict(img)[0][0])\n",
    "prediction = ['Non-Corrosion' if pred_value == 1 else 'Corrosion'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a5b4c682-d0ae-4b61-b2b9-8c9e7513cdda",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " mobilenetv2_1.00_224 (Func  (None, 5, 5, 1280)        2257984   \n",
      " tional)                                                         \n",
      "                                                                 \n",
      " flatten_2 (Flatten)         (None, 32000)             0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 64)                2048064   \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4306113 (16.43 MB)\n",
      "Trainable params: 2048129 (7.81 MB)\n",
      "Non-trainable params: 2257984 (8.61 MB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "157a52c0-6560-4964-9a2c-9f9ba466414f",
   "metadata": {},
   "source": [
    "### Save Model"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3f9f3674-7a84-4608-9f6a-5038c48f83fb",
   "metadata": {},
   "source": [
    "Documentation https://sagemaker-examples.readthedocs.io/en/latest/advanced_functionality/tensorflow_iris_byom/tensorflow_BYOM_iris.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "49daea35-4a3d-4091-a970-2d09ec7af69b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`mobilenetv2_1.00_224_input` is not a valid tf.function parameter name. Sanitizing to `mobilenetv2_1_00_224_input`.\n",
      "WARNING:absl:`mobilenetv2_1.00_224_input` is not a valid tf.function parameter name. Sanitizing to `mobilenetv2_1_00_224_input`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: export/Servo/1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: export/Servo/1/assets\n"
     ]
    }
   ],
   "source": [
    "import tarfile \n",
    "\n",
    "model.save(\"export/Servo/1\")\n",
    "with tarfile.open(\"model.tar.gz\", \"w:gz\") as tar:\n",
    "    tar.add(\"export\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fd0ca553-f39a-4540-95a0-a7c7bddcd4da",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "from sagemaker import session\n",
    "\n",
    "sm_session = session.Session() \n",
    "s3_response = sm_session.upload_data(\"model.tar.gz\", bucket='corrosion-detection-data', key_prefix=\"model\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "189aaa22-a342-40e4-bea0-eec8ac704c85",
   "metadata": {},
   "source": [
    "### Deploy the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cbe785c9-13f3-4723-84ac-6d22327f7346",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-28 05:17:38.420489: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-09-28 05:17:46.613539: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.16.2\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0e9fe831-b7b4-4dbb-a41b-207e6f9113a7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import sagemaker\n",
    "from sagemaker.tensorflow.model import TensorFlowModel\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "# the (default) IAM role you created when creating this notebook\n",
    "role = get_execution_role()\n",
    "\n",
    "bucket_name = 'corrosion-detection-data'\n",
    "\n",
    "# Initialize SageMaker session\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "\n",
    "sagemaker_model = TensorFlowModel(\n",
    "    model_data=f\"s3://{bucket_name}/model/model.tar.gz\",\n",
    "    role=role,\n",
    "    framework_version=\"2.16\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b73d6b9-2c29-483c-a4b8-36f59101cea8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----!"
     ]
    }
   ],
   "source": [
    "endpoint_name = 'corrosion-endpoint'\n",
    "predictor = sagemaker_model.deploy(initial_instance_count=1, instance_type=\"ml.m5.xlarge\", endpoint_name=endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff8e995-46ac-4f1a-b0ae-722daae6084b",
   "metadata": {},
   "source": [
    "### Use the Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "945438e4-8c90-4ca2-8eed-855b103fde44",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import io\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sagemaker import session, Predictor\n",
    "\n",
    "# Assuming you have already deployed your model\n",
    "# Define the endpoint name\n",
    "endpoint_name = \"corrosion-endpoint\"\n",
    "\n",
    "\n",
    "# Define the endpoint name\n",
    "endpoint_name = \"corrosion-endpoint\"\n",
    "\n",
    "# Create a predictor for the deployed model\n",
    "predictor = Predictor(endpoint_name)\n",
    "\n",
    "# Function to load and preprocess an image from a file path\n",
    "def load_and_preprocess_image(image_path):\n",
    "    try:\n",
    "        img = Image.open(image_path)\n",
    "        if img.mode != 'RGB':\n",
    "            img = img.convert('RGB')\n",
    "        img = img.resize((150, 150))\n",
    "        img_array = np.array(img)\n",
    "        img_array = img_array.astype(np.float32) / 255.0\n",
    "        img_array = np.expand_dims(img_array, axis=0)\n",
    "        return img_array\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading image: {e}\")\n",
    "        return None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "629fd81e-2843-48c7-bdf5-d1f55563238f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sagemaker.base_predictor.Predictor at 0x7fa03ee8c1c0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3515fc06-2b63-4b28-8edb-074a17712fe1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Corrosion'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import io\n",
    "import json\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import boto3\n",
    "\n",
    "# Define the endpoint name\n",
    "endpoint_name = \"corrosion-endpoint\"\n",
    "\n",
    "# Function to load and preprocess an image from a file path\n",
    "def load_and_preprocess_image(image_path):\n",
    "    img = Image.open(image_path)\n",
    "    if img.mode != 'RGB':\n",
    "        img = img.convert('RGB')\n",
    "    img = img.resize((150, 150))  # Resize to the expected input shape\n",
    "    img_array = np.array(img)\n",
    "    img_array = img_array.astype(np.float32) / 255.0\n",
    "    img_array = np.expand_dims(img_array, axis=0)  # Add batch dimension\n",
    "    return img_array\n",
    "\n",
    "# Example usage\n",
    "image_path = \"images/pipe.jpg\"  # Replace with your actual image path\n",
    "img = load_and_preprocess_image(image_path=image_path)\n",
    "\n",
    "# Convert the image to a JSON-compatible format\n",
    "json_input = {\n",
    "    \"instances\": img.tolist()  # Convert to a list for JSON serialization\n",
    "}\n",
    "\n",
    "# Use boto3 to send request\n",
    "client = boto3.client('runtime.sagemaker')\n",
    "response = client.invoke_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    Body=json.dumps(json_input),\n",
    "    ContentType='application/json'  # Use application/json for JSON input\n",
    ")\n",
    "\n",
    "# Process the response (this depends on your model's output format)\n",
    "result = response['Body'].read()\n",
    "pred_score = json.loads(result)\n",
    "\n",
    "pred_score = round(pred_score['predictions'][0][0])\n",
    "\n",
    "prediction = ['Non-corrosion' if pred_score == 1 else 'Corrosion'][0]\n",
    "prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c638bec-6b53-4ca0-98ed-acdc7f273875",
   "metadata": {},
   "source": [
    "### Delete the Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "047941c7-687a-431b-8f20-e27a49f812ff",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted endpoint: corrosion-endpoint\n",
      "Error deleting endpoint configuration: An error occurred (ValidationException) when calling the DeleteEndpointConfig operation: Could not find endpoint configuration \"corrosion-endpoint-config\".\n",
      "Error deleting model: An error occurred (ValidationException) when calling the DeleteModel operation: Could not find model \"corrosion-model\".\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "\n",
    "# Create a SageMaker client\n",
    "sagemaker_client = boto3.client('sagemaker')\n",
    "\n",
    "# Define the names\n",
    "endpoint_name = \"corrosion-endpoint\"\n",
    "endpoint_config_name = \"corrosion-endpoint-config\"  # Replace with your endpoint configuration name\n",
    "model_name = \"corrosion-model\"  # Replace with your model name\n",
    "\n",
    "# Step 1: Delete the endpoint\n",
    "try:\n",
    "    sagemaker_client.delete_endpoint(EndpointName=endpoint_name)\n",
    "    print(f\"Deleted endpoint: {endpoint_name}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error deleting endpoint: {e}\")\n",
    "\n",
    "# Step 2: Delete the endpoint configuration\n",
    "try:\n",
    "    sagemaker_client.delete_endpoint_config(EndpointConfigName=endpoint_config_name)\n",
    "    print(f\"Deleted endpoint configuration: {endpoint_config_name}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error deleting endpoint configuration: {e}\")\n",
    "\n",
    "# Step 3: Delete the model\n",
    "try:\n",
    "    sagemaker_client.delete_model(ModelName=model_name)\n",
    "    print(f\"Deleted model: {model_name}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error deleting model: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d03ded9e-d7db-48e5-90c3-0ee74818d66d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow2_p310",
   "language": "python",
   "name": "conda_tensorflow2_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
